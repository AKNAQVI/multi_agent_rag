# ğŸ—ï¸ System Architecture

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Streamlit Frontend                       â”‚
â”‚  (User uploads PDFs, enters queries, views results)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Research Orchestrator                       â”‚
â”‚  (Coordinates all agents and manages workflow)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   RAG System â”‚ â”‚    Agents     â”‚ â”‚  Groq LLM   â”‚
    â”‚   (FAISS)    â”‚ â”‚  (4 agents)   â”‚ â”‚  API        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Details

### 1. Frontend Layer (app.py)
- **Framework**: Streamlit
- **Responsibilities**:
  - File upload interface
  - Query input
  - Progress tracking
  - Results display
  - Session state management

### 2. Orchestration Layer (orchestrator.py)
- **Responsibilities**:
  - Workflow coordination
  - Agent sequencing
  - Progress callbacks
  - Execution logging
  - Error handling

### 3. RAG System (rag_system.py)
- **Components**:
  - PDF text extraction (pypdf)
  - Text chunking (RecursiveCharacterTextSplitter)
  - Embeddings (Sentence Transformers)
  - Vector storage (FAISS)
  - Similarity search

### 4. Agent Layer (agents.py)
Four specialized agents:

#### ğŸ§  Planner Agent
- Input: User query + document count
- Output: List of prioritized tasks
- Method: LLM-based task decomposition

#### ğŸ” Retriever Agent
- Input: Search query
- Output: Relevant document chunks
- Method: FAISS similarity search

#### ğŸ“Š Analyst Agent
- Input: Task + context chunks
- Output: Analysis results
- Methods:
  - Summarization
  - Insight extraction
  - Contradiction detection
  - Trend analysis

#### âœï¸ Reviewer Agent
- Input: All analysis results
- Output: Polished final report
- Method: LLM-based synthesis and editing

### 5. LLM Layer
- **Provider**: Groq
- **Models**: Mixtral 8x7b, LLaMA 3 70B, Gemma 7B
- **Usage**: All agents use same LLM instance

## Data Flow

### Document Processing Flow
```
PDF Upload
    â†“
Extract Text (pypdf)
    â†“
Split into Chunks (1000 tokens, 200 overlap)
    â†“
Generate Embeddings (Sentence Transformers)
    â†“
Store in FAISS Vector DB
    â†“
Ready for Retrieval
```

### Analysis Flow
```
User Query
    â†“
Planner Agent â†’ Creates Tasks [T1, T2, T3, T4]
    â†“
For each Task:
    â†“
    Retriever Agent â†’ Fetch Relevant Chunks
    â†“
    Analyst Agent â†’ Analyze Chunks
    â†“
    Store Result
    â†“
Reviewer Agent â†’ Synthesize All Results
    â†“
Final Report
```

## Key Design Decisions

### 1. In-Memory Vector Store (FAISS)
**Why**: No database setup, fast, perfect for POC
**Trade-off**: Data lost between sessions
**Alternative**: ChromaDB/Pinecle for persistence

### 2. Sentence Transformers for Embeddings
**Model**: all-MiniLM-L6-v2
**Why**: 
- Fast (CPU-friendly)
- Good quality
- Small model size (80MB)
**Alternative**: OpenAI embeddings (costs $$$)

### 3. LangChain for Agents
**Why**: 
- Standard framework
- Easy LLM integration
- Extensible
**Trade-off**: Adds dependency
**Alternative**: Direct API calls

### 4. Four Separate Agents
**Why**: 
- Clear separation of concerns
- True multi-agent architecture
- Demonstrable workflow
**Alternative**: Single agent with complex prompt

### 5. Groq for LLM
**Why**:
- Fast inference
- Free tier available
- Multiple model choices
**Trade-off**: API dependency
**Alternative**: Local LLMs (Ollama)

## Scalability Considerations

### Current Limitations
- In-memory only (no persistence)
- Single-threaded execution
- Limited by Groq rate limits
- CPU-bound embeddings

### Future Enhancements
1. **Persistence**: Add ChromaDB/PostgreSQL
2. **Parallel Processing**: Process tasks concurrently
3. **Caching**: Cache embeddings and results
4. **GPU Support**: Faster embeddings
5. **Streaming**: Stream results as they arrive

## Security & Privacy

### Current Implementation
- âœ… API keys in environment variables
- âœ… Temporary PDF processing
- âœ… No data logging
- âœ… In-memory only (no disk persistence)

### Considerations
- PDFs never sent to third parties (except embedded via Groq)
- No user data stored
- Session state cleared on browser close
- API calls logged by Groq (their privacy policy applies)

## Performance Characteristics

### Typical Timing (5-page PDFs, 3 documents)
- Document processing: 10-30 seconds
- Planning: 2-5 seconds
- Retrieval: <1 second per query
- Analysis (per task): 5-15 seconds
- Review: 10-20 seconds
- **Total**: 30-90 seconds

### Bottlenecks
1. LLM inference (Groq API)
2. Embedding generation (first time)
3. PDF text extraction (large files)

### Optimization Tips
- Use smaller, focused documents
- Choose faster LLM models (Gemma 7B)
- Reduce chunk count (increase chunk_size)
- Limit number of tasks (modify Planner)

## Error Handling

### Implemented
- PDF extraction errors
- API connection failures
- Invalid API keys
- JSON parsing errors
- Empty document handling

### User Experience
- Clear error messages
- Graceful fallbacks
- Progress indicators
- Execution logs for debugging

## Code Organization

```
multi_agent_rag/
â”‚
â”œâ”€â”€ app.py              # Streamlit UI (frontend)
â”œâ”€â”€ orchestrator.py     # Workflow coordinator
â”œâ”€â”€ agents.py           # Four agent classes
â”œâ”€â”€ rag_system.py       # FAISS + embeddings
â”‚
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ .env               # API keys
â”œâ”€â”€ .gitignore         # Git exclusions
â”‚
â”œâ”€â”€ README.md          # Full documentation
â”œâ”€â”€ QUICKSTART.md      # Quick start guide
â””â”€â”€ ARCHITECTURE.md    # This file
```

## Extension Points

### Easy to Add
1. New agent types (e.g., Citation Agent)
2. Different LLM providers
3. Additional file types (DOCX, TXT)
4. Custom analysis tasks
5. Export formats (PDF, DOCX)

### Requires Refactoring
1. Real-time streaming
2. Multi-user support
3. Database persistence
4. Authentication/authorization
5. API endpoint

---

**Architecture designed for clarity, demonstrability, and extensibility**